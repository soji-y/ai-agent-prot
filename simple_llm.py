import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ­ãƒ¼ã‚«ãƒ«LLM
class SimpleLLM:

  # ------------------
  # åˆæœŸåŒ–
  # ------------------
  def __init__(self, model_id_path, system_prompt):
    print("ðŸ”§ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
    self.tokenizer = AutoTokenizer.from_pretrained(model_id_path)

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆåž‹ã¨ãƒ‡ãƒã‚¤ã‚¹ã¯è‡ªå‹•è¨­å®šã€å¿…è¦ãªã‚‰load_in_8bitã‚‚æ¤œè¨Žï¼‰
    self.model = AutoModelForCausalLM.from_pretrained(
        model_id_path,
        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="cuda:0",
    )

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š
    self.system_prompt = system_prompt

    
  # ------------------
  # LLMã§å›žç­”ã‚’ç”Ÿæˆ
  # ------------------
  def generate(self, prompt, sys_use=True):

    # é€ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çµ„ã¿ç«‹ã¦
    messages = []
    if sys_use:
      messages.append({"role": "system", "content": self.system_prompt})
    messages.append({"role": "user", "content": prompt})

    
    # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ãŸæ–‡å­—åˆ—ã«å¤‰æ›
    temp_text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=True)
    
    # ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›
    input_ids = self.tokenizer([temp_text], return_tensors="pt").to(self.model.device)
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
    generated_ids = self.model.generate(
      **input_ids,
      max_new_tokens=512, # ç”Ÿæˆã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€å¤§æ•°
      temperature=0.7, # ç¢ºçŽ‡åˆ†å¸ƒã®å½¢ã‚’èª¿ç¯€ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    )

    # å…¥åŠ›éƒ¨åˆ†ã‚’ã‚¹ãƒ©ã‚¤ã‚¹ã—ã¦å‡ºåŠ›ã®ã¿å–å¾—
    output_ids = generated_ids[0][input_ids.input_ids.shape[1]:].tolist()
      
    # ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’æ–‡å­—åˆ—ã«æˆ»ã™
    content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()

    return content


# -----------------------
# èµ·å‹•: å¤–å´ã®å¯¾è©±ãƒ«ãƒ¼ãƒ—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ã‚„ã‚Šå–ã‚Šã¯ã“ã“ã§ç®¡ç†ï¼‰
# -----------------------
if __name__ == "__main__":
  model_id = "Qwen/Qwen3-4B-Instruct-2507"
  system_prompt = (
    "ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã™ã‚‹å›žç­”ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
  )

  llm = SimpleLLM(model_id, system_prompt)

  print("ðŸ¤– LLMèµ·å‹• â€” ä½•ã‚’é ¼ã¿ã¾ã™ã‹ï¼Ÿï¼ˆ'exit'ã§çµ‚äº†ï¼‰")
  while True:
    goal = input("\nðŸŽ¯ ç›®çš„ã‚’å…¥åŠ›: ")
    if goal.strip().lower() in ("çµ‚äº†", "exit", "quit"):
      print("ðŸ”š çµ‚äº†ã—ã¾ã™ã€‚")
      break

    print("\nðŸ§© LLMãŒå›žç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
    answer = llm.generate(goal)
    
    print(f"\nðŸ’¡ LLMã®æœ€çµ‚å›žç­”:\n{answer}\n")
    print("="*60)
    
# ã€å…¥åŠ›ä¾‹ã€‘
# ã“ã‚“ã«ã¡ã¯ã€‚
# æœ€æ–°ã®æ—¥æœ¬ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚
# 12345678901234567890 * 98765432109876543210 ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚
# > [æ­£è§£] 1219326311370217952237463801111263526900
