import ollama


# ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ­ãƒ¼ã‚«ãƒ«LLM(Ollama)
class SimpleLLMOllama:

  # ------------------
  # åˆæœŸåŒ–
  # ------------------
  def __init__(self, model_id, system_prompt, ollama_host='http://127.0.0.1:11434'):
    self.model_id = model_id
    self.system_prompt = system_prompt # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š
    self.ollama_client = ollama.Client(host=ollama_host)

  # ------------------
  # LLMã§å›žç­”ã‚’ç”Ÿæˆ
  # ------------------
  def generate(self, prompt, sys_use=True):

    # é€ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çµ„ã¿ç«‹ã¦
    messages = []
    if sys_use:
      messages.append({"role": "system", "content": self.system_prompt})
    messages.append({"role": "user", "content": prompt})

    # å›žç­”ã‚’ç”Ÿæˆ
    output_text = self.ollama_client.chat(model=self.model_id, messages=messages)
    
    # å›žç­”ã®æ–‡å­—åˆ—ã‚’å–ã‚Šå‡ºã—
    content = output_text.get("message", {}).get("content", "")

    return content


# -----------------------
# èµ·å‹•: å¤–å´ã®å¯¾è©±ãƒ«ãƒ¼ãƒ—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ã‚„ã‚Šå–ã‚Šã¯ã“ã“ã§ç®¡ç†ï¼‰
# -----------------------
if __name__ == "__main__":
  model_id = "gemma3:4b"

  system_prompt = (
    "ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã™ã‚‹å›žç­”ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
  )

  llm = SimpleLLMOllama(model_id, system_prompt)

  print("ðŸ¤– LLMèµ·å‹• â€” ä½•ã‚’é ¼ã¿ã¾ã™ã‹ï¼Ÿï¼ˆ'exit'ã§çµ‚äº†ï¼‰")
  while True:
    goal = input("\nðŸŽ¯ ç›®çš„ã‚’å…¥åŠ›: ")
    if goal.strip().lower() in ("çµ‚äº†", "exit", "quit"):
      print("ðŸ”š çµ‚äº†ã—ã¾ã™ã€‚")
      break

    print("\nðŸ§© LLMãŒå›žç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
    answer = llm.generate(goal)
    
    print(f"\nðŸ’¡ LLMã®æœ€çµ‚å›žç­”:\n{answer}\n")
    print("="*60)
    
# ã€å…¥åŠ›ä¾‹ã€‘
# ã“ã‚“ã«ã¡ã¯ã€‚
# æœ€æ–°ã®æ—¥æœ¬ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚
# 12345678901234567890  98765432109876543210 ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚
# > [æ­£è§£] 1219326311370217952237463801111263526900
