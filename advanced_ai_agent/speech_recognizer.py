import sys
import torch
import sounddevice as sd
import numpy as np
import queue
import threading
import soundfile as sf
import librosa
from PyQt5.QtWidgets import (
  QApplication, QMainWindow, QTextEdit, QAction, QMenu
)
# pyqtSlot, pyqtSignal, QObject ã‚’è¿½åŠ 
from PyQt5.QtCore import QMetaObject, Q_ARG, Qt, pyqtSlot, QObject, pyqtSignal

# faster-whisper ã‚’ä½¿ç”¨
from faster_whisper import WhisperModel 
# LLM ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from transformers import AutoModelForCausalLM, AutoTokenizer 
from silero_vad import load_silero_vad, get_speech_timestamps, read_audio, save_audio, VADIterator, collect_chunks

# â€” éŸ³å£°å–å¾— + VAD ã®å¼·åŒ–éƒ¨ â€” #
class RealTimeVADRecorder:
  def __init__(self, sample_rate=16000, frame_duration_ms=100, padding_ms=500):
    import queue
    import os
    import urllib.request
    import torch

    self.sample_rate = sample_rate
    self.frame_duration_ms = frame_duration_ms
    self.frame_size = int(sample_rate * frame_duration_ms / 1000)
    self.padding_frames = int(padding_ms / frame_duration_ms)

    # çµ‚äº†ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
    self.stop_frame = 30

    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… VAD running on device: {self.device}")

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ (JIT ãƒ¢ãƒ¼ãƒ‰)
    self.model = load_silero_vad(onnx=False)  # å¿…è¦ãªã‚‰ onnx=True ã‚’æŒ‡å®š
    self.model.to(self.device).eval()

    # # --- âœ… torch.hub.load ã‚’é¿ã‘ã¦æ‰‹å‹•ãƒ­ãƒ¼ãƒ‰ ---
    # model_url = "https://github.com/snakers4/silero-vad/raw/master/files/silero_vad.jit"
    # model_path = "./silero_vad.jit"

    # if not os.path.isfile(model_path):
    #   print("Downloading Silero VAD model...")
    #   urllib.request.urlretrieve(model_url, model_path)

    # self.model = torch.jit.load(model_path, map_location=self.device)
    # self.model.eval()

    # Sileroå…¬å¼ã‹ã‚‰å¿…è¦ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’å–å¾—
    from silero_vad import get_speech_timestamps, collect_chunks, save_audio, read_audio, VADIterator
    self.get_speech_timestamps = get_speech_timestamps
    self.collect_chunks = collect_chunks
    self.save_audio = save_audio
    self.read_audio = read_audio
    self.VADIterator = VADIterator

    self.audio_queue = queue.Queue()
    self.stream = None
    self.running = False
  
  def audio_callback(self, indata, frames, time_info, status):
      if status:
          print("Audio status:", status)
      mono = indata.mean(axis=1)
      self.audio_queue.put(mono.copy())

  def start(self):
      self.stream = sd.InputStream(
          samplerate=self.sample_rate,
          channels=1,
          callback=self.audio_callback,
          blocksize=self.frame_size,
      )
      self.stream.start()
      self.running = True

  def stop(self):
      self.running = False
      if self.stream and self.stream.active:
          try:
              self.stream.stop()
          except Exception as e:
              print(f"Warning: Stream stop failed: {e}")
          try:
              self.stream.close()
          except Exception as e:
              print(f"Warning: Stream close failed: {e}")
      # ã‚­ãƒ¥ãƒ¼ã‚’ã‚¯ãƒªã‚¢
      while not self.audio_queue.empty():
          try:
              self.audio_queue.get_nowait()
          except queue.Empty:
              break

  def read_chunk(self, timeout=None):
      try:
          return self.audio_queue.get(timeout=timeout)
      except queue.Empty:
          return None

  def detect_speech_segment(self):


    ring = []
    voiced = []
    triggered = False
    num_unvoiced = 0
    chunk_buffer = np.zeros(0, dtype=np.float32)

    while True:
      frame = self.read_chunk(timeout=1.0)
      if frame is None:
        # å…¥åŠ›ãŒç„¡ã„å ´åˆã€ã‚ã‚‹ç¨‹åº¦å¾…ã£ã¦ã‚‚éŸ³å£°ãŒæ¥ãªã‘ã‚Œã°çµ‚äº†
        silence_counter += 1
        if silence_counter > 50:  # ç´„50ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†ä½•ã‚‚å…¥åŠ›ãŒç„¡ã‘ã‚Œã°çµ‚äº†
          break
        continue
      silence_counter = 0

      # ãƒ•ãƒ¬ãƒ¼ãƒ çµåˆ
      chunk_buffer = np.concatenate([chunk_buffer, frame])

      while len(chunk_buffer) >= 512:
        current_chunk = chunk_buffer[:512]
        chunk_buffer = chunk_buffer[512:]

        audio_tensor = torch.from_numpy(current_chunk).unsqueeze(0).to(self.device)

        # ç„¡éŸ³ãƒã‚§ãƒƒã‚¯
        if torch.mean(torch.abs(audio_tensor)) < 1e-4:
          is_speech = False
        else:
          try:
            with torch.no_grad():
              is_speech_prob = self.model(audio_tensor, self.sample_rate).item()
            is_speech = is_speech_prob > 0.5
          except ValueError as e:
            if "too short" in str(e):
              continue
            else:
              raise

        # ç™ºè©±ã®é–‹å§‹ï¼çµ‚äº†æ¤œå‡º
        ring.append(current_chunk)
        if len(ring) > self.padding_frames:
          ring.pop(0)

        if not triggered:
          if is_speech:
            triggered = True
            voiced.extend(ring)
            ring.clear()
        else:
          voiced.append(current_chunk)
          if not is_speech:
            num_unvoiced += 1
            if num_unvoiced >= self.stop_frame:
              break  # ğŸ”¸ç™ºè©±çµ‚äº†æ¤œå‡º
          else:
            num_unvoiced = 0

      # ğŸ”¸ç™ºè©±çµ‚äº†æ¤œå‡ºã§breakã—ãŸã‚‰å¤–å´ã‚‚æŠœã‘ã‚‹
      if triggered and num_unvoiced >= self.stop_frame:
        break

    # ğŸ”¸æœ€å¾Œã«éŸ³å£°ãŒæºœã¾ã£ã¦ã„ã‚Œã°è¿”ã™
    if voiced:
      audio = np.concatenate(voiced, axis=0)
      return audio
    else:
      return None
    
# â€” ã‚·ã‚°ãƒŠãƒ«å®šç¾©ã‚¯ãƒ©ã‚¹ â€” #
class LlmSignals(QObject):
  """
  LLM/ASRå‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰MainWindowã¸é€šçŸ¥ã™ã‚‹ãŸã‚ã®ã‚·ã‚°ãƒŠãƒ«ã‚’å®šç¾©
  """
  llm_loaded = pyqtSignal()
  llm_error = pyqtSignal(str) 
  recognized = pyqtSignal(str)
  llm_responded = pyqtSignal(str)

# â€” éŸ³å£°èªè­˜éƒ¨ â€” #
class SpeechRecognizer:
  """
  sounddevice ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«éŸ³å£°ã‚’å–å¾—ã—ã€ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°å¾Œã€
  faster-whisper ã§èªè­˜ã—ã€çµæœã‚’ã‚·ã‚°ãƒŠãƒ«ã§GUIã‚¹ãƒ¬ãƒƒãƒ‰ã«æ¸¡ã™ã€‚
  """
  def __init__(self, signals: LlmSignals, model_name="large-v3", sample_rate=16000, device=None):
    self.signals = signals 
    self.sample_rate = sample_rate
    self.stream = None
    
    if device is None:
      device = "cuda" if torch.cuda.is_available() else "cpu"

    print(f"Loading faster-whisper model: {model_name} on {device}...")
    self.model = WhisperModel(model_name, device=device, compute_type="float16" if device == "cuda" else "int8")
    
    self.q = queue.Queue()
    self.running = False
    self.silence_limit = 1.5
    self.frame_duration = 0.3 # 0.1

    # âœ… ä¿®æ­£: VADRecorderã‚’åˆæœŸåŒ–
    self.vad_recorder = RealTimeVADRecorder(sample_rate=self.sample_rate)

  def audio_callback(self, indata, frames, time, status):
    """sounddeviceã®å…¥åŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹"""
    if status:
      print("Audio status:", status, file=sys.stderr)
    if not self.running:
      return
            
    if indata.shape[1] > 1:
      indata = indata.mean(axis=1)
    
    self.q.put(indata.copy().astype(np.float32))

  def start(self):
    """éŸ³å£°å…¥åŠ›ã‚’é–‹å§‹ã—ã€å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’èµ·å‹•"""
    if self.running:
      return
            
    self.running = True

    # âœ… ä¿®æ­£: VADRecorderã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’é–‹å§‹
    self.vad_recorder.start() 
    
    # âœ… ä¿®æ­£: VADRecorderã‹ã‚‰éŸ³å£°ã‚’å–å¾—ã™ã‚‹ _run ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’èµ·å‹•
    threading.Thread(target=self._run, daemon=True).start()
    # threading.Thread(target=self._run).start() # âœ… ä¿®æ­£å¾Œ (ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚’å¤–ã™)
    
    blocksize = int(self.sample_rate * self.frame_duration)
    self.stream = sd.InputStream(
      channels=1, 
      samplerate=self.sample_rate, 
      callback=self.audio_callback,
      blocksize=blocksize
    )
    self.stream.start()

  def stop(self):
    """éŸ³å£°å…¥åŠ›ã‚’åœæ­¢"""
    self.running = False
    if self.stream:
      self.stream.stop()
      self.stream.close()

  def _run(self):
    """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—ã—ã€ç„¡éŸ³ã§èªè­˜ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹"""
    print("ğŸ™ï¸ éŸ³å£°å…¥åŠ›å¾…æ©Ÿä¸­...")
    # audio_data_list = []
    # silent_frames = 0
    # silence_threshold_frames = int(self.silence_limit / self.frame_duration)

    while self.running:

      # âœ… ä¿®æ­£: VADRecorderã«ç™ºè©±ã®é–‹å§‹ã¨çµ‚äº†ã‚’ä»»ã›ã€çµåˆæ¸ˆã¿ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚‹
      # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯ detect_speech_segment å†…ã® read_chunk(timeout=1.0) ã§å‡¦ç†ã•ã‚Œã‚‹
      full_audio = self.vad_recorder.detect_speech_segment() 
      
      if full_audio is not None:
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ã€èªè­˜å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’èµ·å‹•
        threading.Thread(target=self._process_audio, args=(full_audio,), daemon=True).start()
        
        print("VADæ¤œçŸ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚")
    
      # try:
      #   data = self.q.get(timeout=self.frame_duration)
      #   audio_data_list.append(data)
      #   silent_frames = 0
      # except queue.Empty:
      #   silent_frames += 1
        
      #   if silent_frames > silence_threshold_frames and audio_data_list:
      #     full_audio = np.concatenate(audio_data_list, axis=0)
      #     threading.Thread(target=self._process_audio, args=(full_audio,), daemon=True).start()
          
      #     audio_data_list = []
      #     silent_frames = 0

  def _process_audio(self, audio):
    """éŸ³å£°èªè­˜ã®å®Ÿè¡Œã¨çµæœã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    print("ğŸ§© éŸ³å£°èªè­˜ä¸­...")
    
    segments, info = self.model.transcribe(
      audio, 
      language="ja",
      vad_filter=False, 
    )
    
    text = " ".join([segment.text for segment in segments]).strip()

    if text:
      # ã‚·ã‚°ãƒŠãƒ«ã‚’ç™ºè¡Œ
      self.signals.recognized.emit(text) 

# â€” LLMå¿œç­”éƒ¨ â€” #
class LLMResponder:
  """
  LLMã‚’ä½¿ã£ã¦ã€éŸ³å£°èªè­˜çµæœã«è¿”ç­”ãŒå¿…è¦ã‹åˆ¤å®šã—ã€å¿œç­”ã‚’ç”Ÿæˆ
  """
  # âœ… æœ€æ–°ã®Qwenãƒ¢ãƒ‡ãƒ«IDã‚’ä½¿ç”¨
  def __init__(self, signals: LlmSignals, model_id="Qwen/Qwen3-1.7B", model=None, tokenizer=None):
    self.signals = signals 

    if model:
      self.model = model
      self.tokenizer = tokenizer
    else:
      self.device = "cuda" if torch.cuda.is_available() else "cpu"
      print(f"Loading LLM model: {model_id} on {self.device}...")
      
      self.tokenizer = AutoTokenizer.from_pretrained(model_id)
      self.model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        device_map="auto", 
        torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        trust_remote_code=False 
      )

  def process(self, text):
    """ç™ºè©±ãŒå•ã„ã‹ã‘ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã€å¿…è¦ãªã‚‰å¿œç­”ã‚’ç”Ÿæˆ"""

    # ã‚ãªãŸã¯æ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

    prompt = f"""
æ¬¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ç™ºè©±ã«å¯¾ã—ã¦ã€è³ªå•ã‚„æŒ‡ç¤ºã€ã¾ãŸã¯å¿œç­”ãŒå¿…è¦ãªå†…å®¹ã§ã‚ã‚Œã°ã€ãã®å¿œç­”ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
å˜ãªã‚‹æŒ¨æ‹¶ã‚„ç‹¬ã‚Šè¨€ãªã©ã€å¿œç­”ãŒä¸è¦ãªå ´åˆã¯ã€Œ(ã‚¹ãƒ«ãƒ¼)ã€ã¨ã ã‘è¿”ã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼: {text}
ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: """
    
    inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
    
    outputs = self.model.generate(
      **inputs, 
      max_new_tokens=150, 
      do_sample=True,
      temperature=0.7,
      pad_token_id=self.tokenizer.eos_token_id,
    )
    
    reply = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
    
    if "(ã‚¹ãƒ«ãƒ¼)" in reply or reply.lower().strip() == "(ã‚¹ãƒ«ãƒ¼)":
      print()
      return None
      
    return reply.split("ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:")[-1].strip()

# â€” GUIéƒ¨ â€” #
class MainWindow(QMainWindow):
  
  def __init__(self):
    super().__init__()
    self.setWindowTitle("ğŸ§ éŸ³å£°å…¥åŠ›å¯¾å¿œãƒ­ãƒ¼ã‚«ãƒ«AI")
    self.textbox = QTextEdit()
    self.setCentralWidget(self.textbox)
    self.recognizer = None
    self.llm = None
    self.audio_enabled = False
    
    # ã‚·ã‚°ãƒŠãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    self.signals = LlmSignals()
    self._connect_signals()

    # âœ… ä¿®æ­£: ãƒ¡ã‚½ãƒƒãƒ‰å®šç¾©ãŒçœç•¥ã•ã‚Œã¦ã„ãŸ init_menu ã‚’ã“ã“ã§å‘¼ã³å‡ºã—
    self.init_menu() 
    
    self.textbox.append("ğŸ¤– AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    threading.Thread(target=self._load_llm, daemon=True).start()
    
  # âœ… ä¿®æ­£: init_menu ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†å®šç¾©
  def init_menu(self):
    """å³ã‚¯ãƒªãƒƒã‚¯ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®è¨­å®š"""
    self.menu = QMenu(self)
    self.toggle_action = QAction("éŸ³å£°å…¥åŠ›ã‚’ON", self, checkable=True)
    self.toggle_action.triggered.connect(self.toggle_audio)
    self.toggle_action.setEnabled(False) # ãƒ­ãƒ¼ãƒ‰å®Œäº†ã¾ã§ç„¡åŠ¹
    self.menu.addAction(self.toggle_action)
    
    # âœ… ä¿®æ­£1: ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒãƒªã‚·ãƒ¼ã‚’è¨­å®š
    self.textbox.setContextMenuPolicy(Qt.CustomContextMenu) 
    # âœ… ä¿®æ­£2: ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã®ã‚·ã‚°ãƒŠãƒ«ã«æ¥ç¶š
    self.textbox.customContextMenuRequested.connect(self.show_menu)
        
  def show_menu(self, pos):
    """ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¡¨ç¤º"""
    # self.menu.exec_(self.mapToGlobal(pos))
    """ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¡¨ç¤º"""
    # mapToGlobal ã‚’ä½¿ã£ã¦ã€ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆåº§æ¨™ã‚’ç”»é¢å…¨ä½“åº§æ¨™ã«å¤‰æ›ã—ã¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
    self.menu.exec_(self.textbox.mapToGlobal(pos))
    
  def _connect_signals(self):
    """ã‚·ã‚°ãƒŠãƒ«ã¨ã‚¹ãƒ­ãƒƒãƒˆã‚’æ¥ç¶š"""
    self.signals.llm_loaded.connect(self.on_llm_loaded)
    self.signals.llm_error.connect(self.on_llm_error)
    self.signals.recognized.connect(self.on_recognized_gui_thread)
    self.signals.llm_responded.connect(self.display_llm_response)

  def _load_llm(self):
    """LLMã®ãƒ­ãƒ¼ãƒ‰ (æ™‚é–“ã®ã‹ã‹ã‚‹å‡¦ç†)"""
    try:
      # LLMResponder ã« signals ã‚’æ¸¡ã™
      self.llm = LLMResponder(self.signals) 
      self.signals.llm_loaded.emit() 
    except Exception as e:
      # Qwen/Qwen3-1.7B-Instruct ã¯å¤§å®¹é‡ã§ã‚ã‚‹ãŸã‚ã€GPUç’°å¢ƒè¨­å®šã‚„ãƒ¡ãƒ¢ãƒªä¸è¶³ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚„ã™ã„
      error_msg = f"LLMãƒ­ãƒ¼ãƒ‰ä¸­ã«é‡å¤§ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\nåŸå› : {e}"
      # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã¨ã‚‚ã«ã‚·ã‚°ãƒŠãƒ«ã‚’ç™ºè¡Œ
      self.signals.llm_error.emit(error_msg) 

  @pyqtSlot()
  def on_llm_loaded(self):
    """LLMãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã®GUIæ›´æ–°"""
    self.textbox.append("âœ… AIãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    self.toggle_action.setEnabled(True) 

  @pyqtSlot(str) 
  def on_llm_error(self, error_message):
    """LLMãƒ­ãƒ¼ãƒ‰å¤±æ•—æ™‚ã®GUIæ›´æ–°"""
    self.textbox.append(f"âŒ {error_message}")

  def toggle_audio(self):
    """éŸ³å£°å…¥åŠ›ã®ON/OFFåˆ‡ã‚Šæ›¿ãˆ"""
    if self.llm is None:
      self.toggle_action.setChecked(False)
      self.textbox.append("âš ï¸ AIãƒ¢ãƒ‡ãƒ«ãŒã¾ã ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
      return

    if self.toggle_action.isChecked():
      self.audio_enabled = True
      # SpeechRecognizer ã« signals ã‚’æ¸¡ã™
      self.recognizer = SpeechRecognizer(self.signals) 
      threading.Thread(target=self.recognizer.start, daemon=True).start()
      self.textbox.append("ğŸ™ï¸ éŸ³å£°å…¥åŠ›ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
    else:
      self.audio_enabled = False
      if self.recognizer:
        self.recognizer.stop()
        self.recognizer = None
      self.textbox.append("ğŸ›‘ éŸ³å£°å…¥åŠ›ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚")

  @pyqtSlot(str) 
  def on_recognized_gui_thread(self, text):
    """éŸ³å£°èªè­˜çµæœã‚’GUIã‚¹ãƒ¬ãƒƒãƒ‰ã§å‡¦ç†"""
    self.textbox.append(f"ğŸ‘¤ ã‚ãªãŸ: {text}")
    threading.Thread(target=self._process_llm, args=(text,), daemon=True).start()

  def _process_llm(self, text):
    """LLMã§ã®å¿œç­”ç”Ÿæˆ (æ™‚é–“ã®ã‹ã‹ã‚‹å‡¦ç†)"""
    try:
      response = self.llm.process(text) 
      if response:
        self.signals.llm_responded.emit(response)
    except Exception as e:
      print(f"LLMå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}") 

  @pyqtSlot(str) 
  def display_llm_response(self, response):
    """LLMã®å¿œç­”ã‚’GUIã‚¹ãƒ¬ãƒƒãƒ‰ã§è¡¨ç¤º"""
    self.textbox.append(f"ğŸ¤– AI: {response}")


if __name__ == "__main__":
  app = QApplication(sys.argv)
  win = MainWindow()
  win.resize(600, 400)
  win.show()
  sys.exit(app.exec_())